{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import pyvital.arr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy import signal\n",
    "import scipy as sp\n",
    "import pyvital.filters.abp_ppv as f\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "# df_trks = pd.read_csv('https://api.vitaldb.net/trks')  # read track list\n",
    "# df_cases = pd.read_csv(\"https://api.vitaldb.net/cases\")  # read case information\n",
    "# df_labs = pd.read_csv('https://api.vitaldb.net/labs')  # laboratory results\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import padasip as pa\n",
    "from scipy.signal import find_peaks\n",
    "from tsfresh.feature_extraction.feature_calculators import time_reversal_asymmetry_statistic\n",
    "import math\n",
    "import scipy\n",
    "import scipy.integrate\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import pyvital\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh import extract_features\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import smogn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "from skrvm import RVR\n",
    "#import catboost as cb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import signal\n",
    "import biosppy\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "      <th>1508</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.2</td>\n",
       "      <td>26.8</td>\n",
       "      <td>...</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>72.161797</td>\n",
       "      <td>76.111603</td>\n",
       "      <td>79.073997</td>\n",
       "      <td>82.036301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.2</td>\n",
       "      <td>26.8</td>\n",
       "      <td>...</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>75.124100</td>\n",
       "      <td>77.099098</td>\n",
       "      <td>81.048897</td>\n",
       "      <td>85.986198</td>\n",
       "      <td>89.935997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.2</td>\n",
       "      <td>26.8</td>\n",
       "      <td>...</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>66.237000</td>\n",
       "      <td>66.237000</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>75.124100</td>\n",
       "      <td>78.086502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.2</td>\n",
       "      <td>26.8</td>\n",
       "      <td>...</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>72.161797</td>\n",
       "      <td>75.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.2</td>\n",
       "      <td>26.8</td>\n",
       "      <td>...</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>68.211899</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>66.237000</td>\n",
       "      <td>65.249603</td>\n",
       "      <td>65.249603</td>\n",
       "      <td>66.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26604</th>\n",
       "      <td>26604</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>70.186897</td>\n",
       "      <td>70.186897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26605</th>\n",
       "      <td>26605</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>79.073997</td>\n",
       "      <td>77.099098</td>\n",
       "      <td>75.124199</td>\n",
       "      <td>73.149200</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>69.199402</td>\n",
       "      <td>68.211998</td>\n",
       "      <td>67.224503</td>\n",
       "      <td>66.237099</td>\n",
       "      <td>66.237099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26606</th>\n",
       "      <td>26606</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>86.973602</td>\n",
       "      <td>93.885803</td>\n",
       "      <td>99.810600</td>\n",
       "      <td>102.773003</td>\n",
       "      <td>105.735001</td>\n",
       "      <td>107.709999</td>\n",
       "      <td>107.709999</td>\n",
       "      <td>107.709999</td>\n",
       "      <td>105.735001</td>\n",
       "      <td>103.760002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26607</th>\n",
       "      <td>26607</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>63.274700</td>\n",
       "      <td>64.262100</td>\n",
       "      <td>64.262100</td>\n",
       "      <td>65.249603</td>\n",
       "      <td>65.249603</td>\n",
       "      <td>65.249603</td>\n",
       "      <td>66.237099</td>\n",
       "      <td>66.237099</td>\n",
       "      <td>66.237099</td>\n",
       "      <td>66.237099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26608</th>\n",
       "      <td>26608</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>102.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.7</td>\n",
       "      <td>23.9</td>\n",
       "      <td>...</td>\n",
       "      <td>89.935997</td>\n",
       "      <td>87.961098</td>\n",
       "      <td>84.998703</td>\n",
       "      <td>84.011299</td>\n",
       "      <td>82.036400</td>\n",
       "      <td>79.073997</td>\n",
       "      <td>76.111603</td>\n",
       "      <td>74.136703</td>\n",
       "      <td>71.174301</td>\n",
       "      <td>70.186897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26609 rows Ã— 1510 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       0      1     2      3     4     5    6      7     8  \\\n",
       "0               0  2185.0   91.0   6.5   71.0  76.7  82.0  1.0  169.2  26.8   \n",
       "1               1  2185.0   91.0   6.5   71.0  76.7  82.0  1.0  169.2  26.8   \n",
       "2               2  2185.0   91.0   6.5   71.0  76.7  82.0  1.0  169.2  26.8   \n",
       "3               3  2185.0   91.0   6.5   71.0  76.7  82.0  1.0  169.2  26.8   \n",
       "4               4  2185.0   91.0   6.5   71.0  76.7  82.0  1.0  169.2  26.8   \n",
       "...           ...     ...    ...   ...    ...   ...   ...  ...    ...   ...   \n",
       "26604       26604  4800.0  103.0  10.5  102.0  69.5  34.0  1.0  170.7  23.9   \n",
       "26605       26605  4800.0  103.0  10.5  102.0  69.5  34.0  1.0  170.7  23.9   \n",
       "26606       26606  4800.0  103.0  10.5  102.0  69.5  34.0  1.0  170.7  23.9   \n",
       "26607       26607  4800.0  103.0  10.5  102.0  69.5  34.0  1.0  170.7  23.9   \n",
       "26608       26608  4800.0  103.0  10.5  102.0  69.5  34.0  1.0  170.7  23.9   \n",
       "\n",
       "       ...       1499       1500       1501        1502        1503  \\\n",
       "0      ...  69.199402  68.211899  68.211899   68.211899   68.211899   \n",
       "1      ...  71.174301  71.174301  70.186897   70.186897   71.174301   \n",
       "2      ...  68.211899  67.224503  67.224503   66.237000   66.237000   \n",
       "3      ...  71.174301  69.199402  69.199402   69.199402   68.211899   \n",
       "4      ...  69.199402  68.211899  67.224503   67.224503   67.224503   \n",
       "...    ...        ...        ...        ...         ...         ...   \n",
       "26604  ...  70.186897  70.186897  70.186897   71.174301   71.174301   \n",
       "26605  ...  79.073997  77.099098  75.124199   73.149200   71.174301   \n",
       "26606  ...  86.973602  93.885803  99.810600  102.773003  105.735001   \n",
       "26607  ...  63.274700  64.262100  64.262100   65.249603   65.249603   \n",
       "26608  ...  89.935997  87.961098  84.998703   84.011299   82.036400   \n",
       "\n",
       "             1504        1505        1506        1507        1508  \n",
       "0       69.199402   72.161797   76.111603   79.073997   82.036301  \n",
       "1       75.124100   77.099098   81.048897   85.986198   89.935997  \n",
       "2       67.224503   69.199402   71.174301   75.124100   78.086502  \n",
       "3       68.211899   68.211899   69.199402   72.161797   75.124100  \n",
       "4       67.224503   66.237000   65.249603   65.249603   66.237000  \n",
       "...           ...         ...         ...         ...         ...  \n",
       "26604   71.174301   71.174301   70.186897   70.186897   70.186897  \n",
       "26605   69.199402   68.211998   67.224503   66.237099   66.237099  \n",
       "26606  107.709999  107.709999  107.709999  105.735001  103.760002  \n",
       "26607   65.249603   66.237099   66.237099   66.237099   66.237099  \n",
       "26608   79.073997   76.111603   74.136703   71.174301   70.186897  \n",
       "\n",
       "[26609 rows x 1510 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_SV_500_new.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26609, 1510)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = data.to_numpy()\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_feature = np.array([])\n",
    "train_features = np.array([])\n",
    "cardica_cycles = [10]\n",
    "nested_samples = np.array([])\n",
    "count = 0\n",
    "for cc in cardica_cycles:\n",
    "    for i in range(0,len(data_raw)): \n",
    "        print(i)\n",
    "        if np.isnan(data_raw[i][10:]).any():\n",
    "            print(\"something Happend 4\")\n",
    "            continue\n",
    "        signals, info = nk.eda_process(data_raw[i][10:], sampling_rate=100)\n",
    "        bp_signal = signals['EDA_Raw']\n",
    "        \n",
    "        res = STL(bp_signal, period=2).fit()\n",
    "        valid = True\n",
    "        if (bp_signal > 200).any():\n",
    "            print(\"something Happend 1\")\n",
    "            valid = False\n",
    "        elif(bp_signal < 30).any():\n",
    "            print(\"something Happend 2\")\n",
    "            valid = False\n",
    "        elif (np.abs(np.diff(bp_signal)) > 30).any():  # abrupt change -> noise\n",
    "            print(\"something Happend 3\")\n",
    "            valid = False\n",
    "        elif np.isnan(bp_signal).any():\n",
    "            print(\"something Happend 4\")\n",
    "            valid = False\n",
    "        elif (np.max(bp_signal)-np.min(bp_signal)>70).any():\n",
    "            valid = False\n",
    "        if valid == False:\n",
    "            nested_samples = np.append(nested_samples, i)\n",
    "            print(\"something Happend 111\")\n",
    "            continue\n",
    "        \n",
    "        onsets = np.nan_to_num(info['SCR_Onsets'], nan=1, posinf=1, neginf=1).astype(int)\n",
    "        systolics = np.nan_to_num(info['SCR_Peaks'], nan=1, posinf=1, neginf=1).astype(int)\n",
    "        notches = np.nan_to_num(info['SCR_Recovery'], nan=1, posinf=1, neginf=1).astype(int)\n",
    "        valid = True\n",
    "        if len(onsets) < cc:\n",
    "            valid = False\n",
    "        if len(systolics) < cc:\n",
    "            valid = False\n",
    "        if valid == False:\n",
    "            nested_samples = np.append(nested_samples, i)\n",
    "            #print(\"something Happend 2\")\n",
    "            continue\n",
    "        # find the smallest cardiac cycle and use it as a window for search\n",
    "        window = np.min(np.diff(onsets))\n",
    "        # find the onsets in the raw data\n",
    "        systolics_raw = np.array([])\n",
    "        onsets_raw = np.array([])\n",
    "        notches_raw = np.array([])\n",
    "        for s in onsets[1:]:\n",
    "            systolics_raw = np.append(systolics_raw, s+signals['EDA_Raw'][s:s+window].argmax())\n",
    "        for s in onsets[1:]:\n",
    "            onsets_raw = np.append(onsets_raw, s+signals['EDA_Raw'][s:s+20].argmin())\n",
    "        for s in notches[1:]:\n",
    "            notches_raw = np.append(notches_raw, s + signals['EDA_Raw'][s:s+20].argmin())\n",
    "        systolics_raw = systolics_raw.astype(int)\n",
    "        onsets_raw = onsets_raw.astype(int)\n",
    "        notches_raw = notches_raw.astype(int)\n",
    "        valid = True\n",
    "        if len(onsets_raw) < cc:\n",
    "            valid = False\n",
    "        if len(systolics_raw) < cc:\n",
    "            valid = False\n",
    "        if valid == False:\n",
    "            nested_samples = np.append(nested_samples, i)\n",
    "            print(\"something Happend 3\")\n",
    "            continue\n",
    "        bp_cycle = signals['EDA_Raw'][onsets_raw[-cc]:onsets_raw[-1]]\n",
    "        systolics_raw = systolics_raw[-cc:-1]\n",
    "        onsets_raw = onsets_raw[-cc:-1]\n",
    "        notches_raw = notches_raw[-cc:-1]\n",
    "        ave = np.average(np.diff(bp_cycle))\n",
    "        valid = True\n",
    "        if np.max(bp_cycle) - np.min(bp_cycle) < 30:\n",
    "            valid = False\n",
    "        # elif ave > 0.1:\n",
    "        #     valid = False\n",
    "        if valid == False:\n",
    "            nested_samples = np.append(nested_samples, i)\n",
    "            print(\"something Happend 4\")\n",
    "            continue\n",
    "        \n",
    "        SBP = bp_cycle[systolics_raw]\n",
    "        DBP = bp_cycle[onsets_raw]\n",
    "        Pulse_pressure = SBP.values - DBP.values\n",
    "        MAP = DBP + (Pulse_pressure/3)\n",
    "        T_cycle = np.diff(onsets_raw)/100\n",
    "        T_SP = np.abs(np.subtract(onsets_raw, systolics_raw))/100\n",
    "        T_SN = np.abs(np.subtract(systolics_raw, notches_raw))/100\n",
    "        T_DP = np.abs(np.subtract(onsets_raw, notches_raw))/100\n",
    "        HR = 60/T_cycle\n",
    "        Windkessel = SBP.values - DBP.values\n",
    "        Windkessel_RC = (MAP.values[1:]/T_cycle)*np.log(SBP.values[1:]/DBP.values[1:])\n",
    "        Liljestrand_Zander = ((SBP.values-DBP.values)/(SBP.values+DBP.values))\n",
    "        Herd = MAP.values-DBP.values\n",
    "        Pressure_RMS = np.array([])\n",
    "        Systolic_area = np.array([])\n",
    "        Systolic_area_Kouchoukos_correction = np.array([])\n",
    "        \n",
    "        for n in range(0, len(onsets_raw)):\n",
    "            if len(bp_cycle.loc[onsets_raw[n]:notches_raw[n]]) == 0:\n",
    "                continue\n",
    "            Pressure_RMS = np.append(Pressure_RMS,math.sqrt((1/(100*len(bp_cycle.loc[onsets_raw[n]:notches_raw[n]])))*scipy.integrate.simps((bp_cycle.loc[onsets_raw[n]:notches_raw[n]])**2, dx=1/100)))\n",
    "            Systolic_area = np.append(Systolic_area, scipy.integrate.simps(bp_cycle.loc[onsets_raw[n]:notches_raw[n]], dx=1/100))\n",
    "            Systolic_area_Kouchoukos_correction = np.append(Systolic_area_Kouchoukos_correction, scipy.integrate.simps((1+(T_SP[n]/T_DP[n]))*bp_cycle.loc[onsets_raw[n]:notches_raw[n]], dx=1/100))\n",
    "        \n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][1])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][2])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][3])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][4])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][5])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][6])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][7])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][8])\n",
    "        extracted_feature = np.append(extracted_feature, data_raw[i][9])\n",
    "\n",
    "        extracted_feature = np.append(extracted_feature, HR)\n",
    "        extracted_feature = np.append(extracted_feature, T_SP)\n",
    "        extracted_feature = np.append(extracted_feature, T_SN)\n",
    "        extracted_feature = np.append(extracted_feature, T_DP)\n",
    "        extracted_feature = np.append(extracted_feature, T_cycle)\n",
    "        extracted_feature = np.append(extracted_feature, T_cycle)\n",
    "        extracted_feature = np.append(extracted_feature, SBP)\n",
    "        extracted_feature = np.append(extracted_feature, DBP)\n",
    "        extracted_feature = np.append(extracted_feature, Pulse_pressure)\n",
    "        extracted_feature = np.append(extracted_feature, MAP)\n",
    "        ############\n",
    "        extracted_feature = np.append(extracted_feature, Windkessel)\n",
    "        extracted_feature = np.append(extracted_feature, Windkessel_RC)\n",
    "        extracted_feature = np.append(extracted_feature, Liljestrand_Zander)\n",
    "        extracted_feature = np.append(extracted_feature, Herd)\n",
    "        extracted_feature = np.append(extracted_feature, Pressure_RMS)\n",
    "        extracted_feature = np.append(extracted_feature, Systolic_area)\n",
    "        extracted_feature = np.append(extracted_feature, Systolic_area_Kouchoukos_correction)\n",
    "        #############\n",
    "        seasonalitje = res.seasonal\n",
    "        trend_mean = np.mean(res.trend)\n",
    "        trend_std = np.std(res.trend)\n",
    "        seasonal_mean = np.mean(res.seasonal)\n",
    "        seasonal_std = np.std(res.seasonal)\n",
    "        BP_mean = np.mean(signals['EDA_Raw'])\n",
    "        BP_std = np.std(signals['EDA_Raw'])\n",
    "        \n",
    "        extracted_feature = np.append(extracted_feature, trend_mean)\n",
    "        extracted_feature = np.append(extracted_feature, trend_std)\n",
    "        extracted_feature = np.append(extracted_feature, seasonal_mean)\n",
    "        extracted_feature = np.append(extracted_feature, seasonal_std)\n",
    "        extracted_feature = np.append(extracted_feature, BP_mean)\n",
    "        extracted_feature = np.append(extracted_feature, BP_std)\n",
    "        \n",
    "        df = pd.DataFrame({'value': signals['EDA_Raw']})\n",
    "        df['id'] = 1\n",
    "        features = extract_features(df, column_id='id', column_sort=None, column_kind=None, column_value='value')\n",
    "        extracted_feature = np.append(extracted_feature, features['value__variance_larger_than_standard_deviation'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__abs_energy'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__absolute_maximum'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__agg_autocorrelation__f_agg_\"mean\"__maxlag_40'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__absolute_sum_of_changes'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__sum_values'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__mean_abs_change'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__mean_change'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__mean_second_derivative_central'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__median'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__mean'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__length'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__standard_deviation'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__variation_coefficient'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__variance'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__skewness'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__root_mean_square'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_0'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_1'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_2'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_3'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_4'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_5'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_6'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_7'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_8'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__autocorrelation__lag_9'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_0'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_1'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_2'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_3'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_4'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_5'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_6'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_7'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_8'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__partial_autocorrelation__lag_9'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__linear_trend__attr_\"pvalue\"'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__linear_trend__attr_\"rvalue\"'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__linear_trend__attr_\"intercept\"'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__linear_trend__attr_\"slope\"'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__linear_trend__attr_\"stderr\"'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__fourier_entropy__bins_2'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__fourier_entropy__bins_3'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__fourier_entropy__bins_5'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__fourier_entropy__bins_10'][1])\n",
    "        extracted_feature = np.append(extracted_feature, features['value__fourier_entropy__bins_100'][1])\n",
    "        if i == 0:\n",
    "            train_features = extracted_feature\n",
    "            extracted_feature = []\n",
    "        else:\n",
    "            train_features = np.vstack([train_features, extracted_feature])\n",
    "            extracted_feature = []\n",
    "        \n",
    "    df = pd.DataFrame(train_features)\n",
    "\n",
    "df.to_csv('features_vitaldb.csv', encoding = 'utf-8-sig') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Unnamed: 0','ID','HR', 'CO','SV','W', 'Age', 'Sex', 'H', 'BMI',\n",
    "         'HR1','HR2','HR3','HR4','HR5','HR6','HR7','HR8',\n",
    "         'T_SP1','T_SP2','T_SP3','T_SP4','T_SP5','T_SP6','T_SP7','T_SP8',\n",
    "         'T_SN1','T_SN2','T_SN3','T_SN4','T_SN5','T_SN6','T_SN7','T_SN8',\n",
    "         'T_DP1','T_DP2','T_DP3','T_DP4','T_DP5','T_DP6','T_DP7','T_DP8',\n",
    "         'T_C1','T_C2','T_C3','T_C4','T_C5','T_C6','T_C7','T_C8','T_C9',\n",
    "         'T_C11','T_C12','T_C13','T_C14','T_C15','T_C16','T_C17','T_C18','T_C19',\n",
    "         'SBP1','SBP2','SBP3','SBP4','SBP5','SBP6','SBP7','SBP8','SBP9',\n",
    "         'DBP1','DBP2','DBP3','DBP4','DBP5','DBP6','DBP7','DBP8','DBP9',\n",
    "         'pp1','pp2','pp3','pp4','pp5','pp6','pp7','pp8','pp9',\n",
    "         'MAP1','MAP2','MAP3','MAP4','MAP5','MAP6','MAP7','MAP8','MAP9',\n",
    "         'wind1','wind2','wind3','wind4','wind5','wind6','wind7','wind8','wind9',\n",
    "         'wind_RC1','wind_RC2','wind_RC3','wind_RC4','wind_RC5','wind_RC6','wind_RC7','wind_RC8','wind_RC9',\n",
    "         'li_za1','li_za2','li_za3','li_za4','li_za5','li_za6','li_za7','li_za8','li_za9',\n",
    "         'Herd1','Herd2','Herd3','Herd4','Herd5','Herd6','Herd7','Herd8','Herd9',\n",
    "         'P_rms1','P_rms2','P_rms3','P_rms4','P_rms5','P_rms6','P_rms7','P_rms8','P_rms9',\n",
    "         'sys_area1','sys_area2','sys_area3','sys_area4','sys_area5','sys_area6','sys_area7','sys_area8','sys_area9',\n",
    "         'sys_area_kc1','sys_area_kc2','sys_area_kc3','sys_area_kc4','sys_area_kc5','sys_area_kc6','sys_area_kc7','sys_area_kc8','sys_area_kc9',\n",
    "         'trend_m', 'trend_std', 'seasonal_m', 'seasonal_std', 'BP_mean', 'BP_std',\n",
    "         'var_larger_std','abs_energy','abs_max', 'agg_acc', 'abs_sum_changes',\n",
    "         'sum_value', 'm_abs_change','m_change', 'm_s_derivative_c','median', 'mean',\n",
    "         'length', 'std', 'vari_coeff', 'var', 'skewness', 'rms', 'ac0', 'ac1',\n",
    "         'ac2','ac3','ac4','ac5', 'ac6', 'ac7', 'ac8', 'ac9', 'pac0', 'pac1', 'pac2',\n",
    "         'pac3', 'pac4', 'pac5', 'pac6', 'pac7', 'pac8', 'pac9', 'lt_p_value',\n",
    "         'lt_r_value', 'lt_intercept', 'lt_slope', 'lt_stderr', 'f_entropy2', 'f_entropy3',\n",
    "         'f_entropy5', 'f_entropy10','f_entropy100']\n",
    "\n",
    "df.columns = label\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e7743e1e9445e066414ab2ce6c7de6e63f997835c955f4d5282448866e547bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
